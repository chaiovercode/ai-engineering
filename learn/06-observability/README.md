# AI Observability

> See what your AI is actually doing.

## What is Observability?

**Observability** = The ability to understand what's happening inside your AI system by looking at its outputs.

Think of it like a car dashboard:
- Speed (latency)
- Fuel level (cost)
- Engine warning lights (errors)
- Trip computer (metrics)

Without observability, your AI is a **black box**:

```
User Question â”€â”€â–º  [??? MYSTERY ???]  â”€â”€â–º Answer

With observability:

User Question â”€â”€â–º [Step 1] â”€â”€â–º [Step 2] â”€â”€â–º [Step 3] â”€â”€â–º Answer
                    â”‚            â”‚            â”‚
                    â–¼            â–¼            â–¼
                 Logged       Logged       Logged
                 2.3 sec      1.1 sec      0.8 sec
                 500 tokens   200 tokens   150 tokens
```

---

## Why Does This Matter?

### The Problem

AI systems are unpredictable. Same question, different responses. Sometimes wrong answers.

**Common issues:**
- "Why did the AI say that?"
- "Why is this so slow?"
- "How much is this costing me?"
- "Which part is broken?"

### The Solution

Track everything. Know exactly what happened.

---

## LangSmith: The AI Dashboard

**LangSmith** is an observability platform for AI apps (made by the LangChain team).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGSMITH DASHBOARD                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ðŸ“Š Metrics                                              â”‚
â”‚  â”œâ”€â”€ Total requests: 1,234                              â”‚
â”‚  â”œâ”€â”€ Avg latency: 2.3s                                  â”‚
â”‚  â”œâ”€â”€ Error rate: 0.5%                                   â”‚
â”‚  â””â”€â”€ Total cost: $12.34                                 â”‚
â”‚                                                          â”‚
â”‚  ðŸ” Recent Traces                                        â”‚
â”‚  â”œâ”€â”€ "What is AI?" - 1.2s - Success                     â”‚
â”‚  â”œâ”€â”€ "Explain quantum" - 3.4s - Success                 â”‚
â”‚  â””â”€â”€ "Write code..." - ERROR - Token limit              â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Concepts

### 1. Traces (The Full Journey)

A **trace** is the complete record of one request.

```
Trace: "Write a blog post about AI"

â”œâ”€â”€ START (user submits request)
â”‚
â”œâ”€â”€ Step 1: Researcher
â”‚   â”œâ”€â”€ Input: "AI trends"
â”‚   â”œâ”€â”€ Output: "Found 5 key points..."
â”‚   â”œâ”€â”€ Time: 3.2 seconds
â”‚   â””â”€â”€ Tokens: 450
â”‚
â”œâ”€â”€ Step 2: Writer
â”‚   â”œâ”€â”€ Input: "5 key points..."
â”‚   â”œâ”€â”€ Output: "AI is transforming..."
â”‚   â”œâ”€â”€ Time: 5.1 seconds
â”‚   â””â”€â”€ Tokens: 1,200
â”‚
â”œâ”€â”€ Step 3: Editor
â”‚   â”œâ”€â”€ Input: "AI is transforming..."
â”‚   â”œâ”€â”€ Output: "Final polished article..."
â”‚   â”œâ”€â”€ Time: 2.3 seconds
â”‚   â””â”€â”€ Tokens: 800
â”‚
â””â”€â”€ END (total: 10.6 seconds, 2,450 tokens)
```

---

### 2. Runs (Individual Steps)

Each step in a trace is a **run**.

```
Trace (one user request)
â”œâ”€â”€ Run: Researcher (one AI call)
â”œâ”€â”€ Run: Writer (one AI call)
â””â”€â”€ Run: Editor (one AI call)
```

---

### 3. Metrics (The Numbers)

What LangSmith tracks automatically:

| Metric | What It Means | Why It Matters |
|--------|---------------|----------------|
| **Latency** | How long it took | User experience |
| **Tokens** | Input + output tokens | Cost |
| **Cost** | $ spent on API calls | Budget |
| **Errors** | Failed requests | Reliability |
| **Feedback** | Quality scores | Improvement |

---

## Latency Percentiles Explained

You'll see P50, P95, P99 - here's what they mean:

```
P50 (Median): Half of requests are faster than this
P95: 95% of requests are faster than this
P99: 99% of requests are faster than this
```

**Example:**
```
P50 = 2 seconds  â† Typical experience
P95 = 5 seconds  â† Occasional slow request
P99 = 15 seconds â† Rare worst case (1 in 100)
```

**Why P99 matters:** That 1 in 100 user having a bad experience will complain!

---

## Setting Up LangSmith

It's just environment variables:

```bash
# In your .env file
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-key-here
LANGCHAIN_PROJECT=my-ai-app
```

That's it! All LangChain/LangGraph calls are now traced automatically.

---

## Debugging with LangSmith

### Scenario: Slow Response

```
User complains: "Your AI took forever!"

Step 1: Open LangSmith
Step 2: Find the trace
Step 3: See the breakdown:

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Trace: 45 seconds total              â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ Researcher: 3 sec âœ“                  â”‚
        â”‚ Writer: 5 sec âœ“                      â”‚
        â”‚ Fact-Checker: 35 sec â† PROBLEM!      â”‚
        â”‚   â””â”€â”€ Web search: 30 sec â† ROOT CAUSEâ”‚
        â”‚ Editor: 2 sec âœ“                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Fix: Add timeout to web search
```

---

### Scenario: Wrong Answer

```
User complains: "The AI made something up!"

Step 1: Find the trace
Step 2: Look at the inputs/outputs:

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Writer Input:                        â”‚
        â”‚ "Write about Python 3.12"            â”‚
        â”‚                                      â”‚
        â”‚ Fact-Checker Output:                 â”‚
        â”‚ "Claims verified: 2/5" â† PROBLEM!    â”‚
        â”‚ - Made up feature names              â”‚
        â”‚ - Wrong release date                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Fix: Improve fact-checker prompt
```

---

## RAG Evaluation Metrics

For RAG (document-based AI), track these quality scores:

### Faithfulness

**Is the answer based on the documents, or did AI make it up?**

```
Context: "Python was created in 1991"

Good answer: "Python was created in 1991"
Faithfulness: 100% âœ“

Bad answer: "Python was created in 1991 and is the most popular language"
Faithfulness: 50% âœ— (second claim not in context)
```

---

### Answer Relevance

**Did the AI actually answer the question?**

```
Question: "How do I install Python?"

Good answer: "Download from python.org and run the installer"
Relevance: High âœ“

Bad answer: "Python is a programming language created by..."
Relevance: Low âœ— (didn't answer the question)
```

---

### Context Precision

**Did we retrieve the RIGHT documents?**

```
Question: "What's the return policy?"

Retrieved:
1. Return Policy document âœ“ Relevant
2. Shipping FAQ âœ— Not relevant
3. About Us page âœ— Not relevant

Precision: 1/3 = 33% (too much junk!)
```

---

### Context Recall

**Did we find ALL the needed documents?**

```
Question: "What are all the payment methods?"

Needed documents: [Credit Card, PayPal, Bank Transfer]
Retrieved: [Credit Card, PayPal]

Recall: 2/3 = 67% (missed Bank Transfer doc!)
```

---

## Quick Reference

### What to Monitor

| Metric | Alert If | Action |
|--------|----------|--------|
| P99 Latency | > 30 seconds | Optimize slow steps |
| Error Rate | > 5% | Check logs, fix bugs |
| Faithfulness | < 80% | Improve RAG/prompts |
| Daily Cost | > $100 | Review token usage |

---

### Debugging Workflow

```
1. FIND the problem trace
   â””â”€â”€ Filter by: errors, high latency, low scores

2. EXPAND the trace
   â””â”€â”€ See each step's inputs and outputs

3. IDENTIFY the issue
   â””â”€â”€ Which step failed? Why?

4. FIX and verify
   â””â”€â”€ Make changes, check new traces
```

---

## Try It Yourself

```bash
cd 06-observability
pip install -r requirements.txt

# Set up LangSmith (free tier available)
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=your-key

# Run examples
python 01_langsmith_basics.py
python 04_rag_evaluation.py

# Check smith.langchain.com to see your traces!
```

---

## Key Takeaways

1. **Observability = See inside your AI** (not a black box)
2. **Traces show the full journey** of each request
3. **Latency, tokens, cost** - track the numbers
4. **P50/P95/P99** - understand different user experiences
5. **RAG metrics** - faithfulness, relevance, precision, recall
6. **LangSmith** - automatic tracing with environment variables

---

## You've Completed the Learning Path! ðŸŽ‰

You now understand:
- âœ… How LLMs work (fundamentals)
- âœ… How to write effective prompts
- âœ… How to use LLM APIs
- âœ… How to build AI agents
- âœ… How to orchestrate multiple agents
- âœ… How to monitor and debug AI systems

**Next:** Check out the `apps/` folder for real-world examples!
